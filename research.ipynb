{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from typing import Any\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "np.random.seed(15)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    def __init__(self, dataset_name,\n",
    "                 mr = '../../../../data/MR-rt-polaritydata/rt-polaritydata/',\n",
    "                 sst = '../../../../data/SST-stanfordSentimentTreebank/',\n",
    "                 subj = '../../../../data/Subj-rotten_imdb/',\n",
    "                 trec = '../../../../data/TREC-Trec6/',\n",
    "                 cr = '../../../../data/CR-customer review data/',\n",
    "                 mpqa = '../../../../data/MRQA-database.mpqa.1.2/'):\n",
    "        \n",
    "        self.mr, self.sst, self.subj, self.trec, self.cr, self.mpqa = mr, sst, subj, trec, cr, mpqa\n",
    "        self.data = []\n",
    "        assert dataset_name in ['MR', 'SST1', 'SST2', 'Subj', 'TREC', 'CR', 'MPQA']\n",
    "        if dataset_name == 'MR':\n",
    "            self.load_mr_data()\n",
    "            \n",
    "        if dataset_name == 'SST1':\n",
    "            self.load_sst1_data()\n",
    "            \n",
    "        if dataset_name == 'SST2':\n",
    "            self.load_sst2_data()\n",
    "            \n",
    "        if dataset_name == 'Subj':\n",
    "            self.load_subj_data()\n",
    "            \n",
    "        if dataset_name == 'TREC':\n",
    "            self.load_trec_data()\n",
    "            \n",
    "        if dataset_name == 'CR':\n",
    "            self.load_cr_data()\n",
    "            \n",
    "        if dataset_name == 'MPQA':\n",
    "            self.load_mpqa_data()\n",
    "    \n",
    "    def load_data_in_kfolds(self, x, y, folds=10):\n",
    "        kf = KFold(n_splits=10, shuffle=True)\n",
    "        kf.get_n_splits(x)\n",
    "        for train_index, test_index in kf.split(x):\n",
    "\n",
    "            self.data.append({'x_train': [x[i] for i in train_index], \n",
    "                              'y_train': [y[i] for i in train_index], \n",
    "                              'x_test': [x[i] for i in test_index], \n",
    "                              'y_test': [y[i] for i in test_index]})\n",
    "        \n",
    "    def load_mr_data(self):\n",
    "        with open(self.mr + '/rt-polarity.pos', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            pos_statements = f.read().split('\\n')\n",
    "        with open(self.mr + '/rt-polarity.neg', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            neg_statements = f.read().split('\\n')\n",
    "        x = pos_statements + neg_statements\n",
    "        y = ['pos'] * len(pos_statements) + ['neg'] * len(neg_statements)\n",
    "        self.load_data_in_kfolds(x, y)\n",
    "        \n",
    "    def load_sst1_data(self):\n",
    "        sentences = pd.read_csv(self.sst + '/datasetSentences.txt', sep='\\t')\n",
    "        sentences_type = pd.read_csv(self.sst + '/datasetSplit.txt', sep=',')\n",
    "        phase_labels = pd.read_csv(self.sst + '/sentiment_labels.txt', sep='|')\n",
    "        phase_dict = pd.read_csv(self.sst + '/dictionary.txt', sep='|', \n",
    "                                 header=None, \n",
    "                                 names=['phase', 'phase_id'])\n",
    "        \n",
    "        data = pd.merge(sentences, sentences_type, on='sentence_index', how='left')\n",
    "        data = pd.merge(data, phase_dict, left_on='sentence', right_on='phase', how='left')\n",
    "        data = pd.merge(data, phase_labels, left_on='phase_id', right_on='phrase ids', how='left')\n",
    "        \n",
    "        sentiment = []\n",
    "        for index, dp in data.iterrows():\n",
    "            if dp['sentiment values'] <= 0.20:\n",
    "                sentiment.append('very negative')\n",
    "            elif dp['sentiment values'] > 0.20 and dp['sentiment values'] <= 0.40:\n",
    "                sentiment.append('negative')\n",
    "            elif dp['sentiment values'] > 0.40 and dp['sentiment values'] <= 0.60:\n",
    "                sentiment.append('neutral')\n",
    "            elif dp['sentiment values'] > 0.60 and dp['sentiment values'] <= 0.80:\n",
    "                sentiment.append('positive')\n",
    "            elif dp['sentiment values'] > 0.80:\n",
    "                sentiment.append('very positive')\n",
    "            else:\n",
    "                sentiment.append(np.NaN)\n",
    "\n",
    "        data['sentiment'] = sentiment\n",
    "        data.dropna(subset=['sentence', 'sentiment'], inplace=True)\n",
    "        self.data.append({'x_train': data[data.splitset_label.isin([1, 3])]['sentence'].tolist(),\n",
    "                          'y_train': data[data.splitset_label.isin([1, 3])]['sentiment'].tolist(),\n",
    "                          'x_test': data[data.splitset_label == 2]['sentence'].tolist(),\n",
    "                          'y_test': data[data.splitset_label == 2]['sentiment'].tolist()})\n",
    "        \n",
    "    def load_sst2_data(self):\n",
    "        sentences = pd.read_csv(self.sst + '/datasetSentences.txt', sep='\\t')\n",
    "        sentences_type = pd.read_csv(self.sst + '/datasetSplit.txt', sep=',')\n",
    "        phase_labels = pd.read_csv(self.sst + '/sentiment_labels.txt', sep='|')\n",
    "        phase_dict = pd.read_csv(self.sst + '/dictionary.txt', sep='|', \n",
    "                                 header=None, \n",
    "                                 names=['phase', 'phase_id'])\n",
    "        \n",
    "        data = pd.merge(sentences, sentences_type, on='sentence_index', how='left')\n",
    "        data = pd.merge(data, phase_dict, left_on='sentence', right_on='phase', how='left')\n",
    "        data = pd.merge(data, phase_labels, left_on='phase_id', right_on='phrase ids', how='left')\n",
    "        \n",
    "        sentiment = []\n",
    "        for index, dp in data.iterrows():\n",
    "            if dp['sentiment values'] <= 0.40:\n",
    "                sentiment.append('negative')\n",
    "            elif dp['sentiment values'] > 0.60:\n",
    "                sentiment.append('positive')\n",
    "            else:\n",
    "                sentiment.append(np.NaN)\n",
    "\n",
    "        data['sentiment'] = sentiment\n",
    "        data.dropna(subset=['sentence', 'sentiment'], inplace=True)\n",
    "        self.data.append({'x_train': data[data.splitset_label.isin([1, 3])]['sentence'].tolist(),\n",
    "                          'y_train': data[data.splitset_label.isin([1, 3])]['sentiment'].tolist(),\n",
    "                          'x_test': data[data.splitset_label == 2]['sentence'].tolist(),\n",
    "                          'y_test': data[data.splitset_label == 2]['sentiment'].tolist()})\n",
    "        \n",
    "    def load_subj_data(self):\n",
    "        with open(self.subj + '/plot.tok.gt9.5000', 'r',  encoding='utf-8', errors='ignore') as f:\n",
    "            subjective_sentences = f.readlines()\n",
    "        with open(self.subj + '/quote.tok.gt9.5000', 'r',  encoding='utf-8', errors='ignore') as f:\n",
    "            objective_sentences = f.readlines()\n",
    "\n",
    "        x = subjective_sentences + objective_sentences\n",
    "        y = ['subj'] * len(subjective_sentences) + ['obj'] * len(objective_sentences)\n",
    "        self.load_data_in_kfolds(x, y)\n",
    "        \n",
    "    def load_trec_data(self):\n",
    "        data_train = pd.read_csv(self.trec + '/train_5500.label.txt', \n",
    "                sep=\":\", encoding='latin8', header=None, names=['Topic', 'Sentence'])\n",
    "        data_test = pd.read_csv(self.trec + '/TREC_10.label.txt',\n",
    "                sep=\":\", encoding='latin8', header=None, names=['Topic', 'Sentence'])\n",
    "        \n",
    "        self.data.append({'x_train': data_train['Sentence'].tolist(),\n",
    "                          'y_train': data_train['Topic'].tolist(),\n",
    "                          'x_test': data_test['Sentence'].tolist(),\n",
    "                          'y_test': data_test['Topic'].tolist()})\n",
    "        \n",
    "    def load_cr_data(self):\n",
    "        reviews = []\n",
    "        product_type = []\n",
    "\n",
    "        with open(self.cr + '/Canon G3.txt', \n",
    "                  'r',  encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.readlines()[11:]    \n",
    "            for text in data:\n",
    "                reviews.append(text.strip().split('##')[-1].replace('[t]', ''))\n",
    "                product_type.append('camera')\n",
    "\n",
    "        with open(self.cr + '/Apex AD2600 Progressive-scan DVD player.txt', \n",
    "                  'r',  encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.readlines()[11:]    \n",
    "            for text in data:\n",
    "                reviews.append(text.strip().split('##')[-1].replace('[t]', ''))\n",
    "                product_type.append('mp3s etc')\n",
    "\n",
    "        with open(self.cr + '/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt', \n",
    "                  'r',  encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.readlines()[11:]    \n",
    "            for text in data:\n",
    "                reviews.append(text.strip().split('##')[-1].replace('[t]', ''))\n",
    "                product_type.append('mp3s etc')\n",
    "\n",
    "        with open(self.cr + '/Nikon coolpix 4300.txt', \n",
    "                  'r',  encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.readlines()[11:]    \n",
    "            for text in data:\n",
    "                reviews.append(text.strip().split('##')[-1].replace('[t]', ''))\n",
    "                product_type.append('camera')\n",
    "\n",
    "        with open(self.cr + '/Nokia 6610.txt', \n",
    "                  'r',  encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.readlines()[11:]    \n",
    "            for text in data:\n",
    "                reviews.append(text.strip().split('##')[-1].replace('[t]', ''))\n",
    "                product_type.append('mp3s etc')\n",
    "        x = reviews\n",
    "        y = product_type\n",
    "        self.load_data_in_kfolds(x, y)\n",
    "        \n",
    "    def load_mpqa_data(self):\n",
    "        text, sentiment = [], []\n",
    "        with open(self.mpqa + '/mpqa.neg.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.readlines()\n",
    "            text = text + data\n",
    "            sentiment = sentiment + ['neg'] * len(data)\n",
    "        \n",
    "        with open(self.mpqa + '/mpqa.pos.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.readlines()\n",
    "            text = text + data\n",
    "            sentiment = sentiment + ['pos'] * len(data)\n",
    "        x = text\n",
    "        y = sentiment\n",
    "        self.load_data_in_kfolds(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessData():\n",
    "    def __init__(self, use_pretrained_vector=False,\n",
    "                 w2v_path='../../../../data/google.news.word2vec/GoogleNews-vectors-negative300.bin'):\n",
    "        self.word2vec = None\n",
    "        if use_pretrained_vector:\n",
    "            print('Loading Word2vec')\n",
    "            self.word2vec = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "        self.re_word_tokenizer = re.compile(r\"\\w+\", re.I)\n",
    "        self.reset()\n",
    "\n",
    "    def set_dataset_name(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "    def set_maximum_sentence_length(self, sentences):\n",
    "        sentences_tok = self.tokenize_sentences(sentences)\n",
    "        self.max_sen_len = np.max([len(s) for s in sentences_tok])\n",
    "\n",
    "    def reset(self):\n",
    "        self.dictionary = {}\n",
    "        self.class2index = {}\n",
    "        self.index2class = {}\n",
    "        self.classCount = 0\n",
    "        self.word2index = {'unk': 0}\n",
    "        self.wordcount = {'unk': 0}\n",
    "        self.index2word = {0: 'unk'}\n",
    "        self.wordCount = 0\n",
    "        self.wordCount_w2v = 0\n",
    "        self.weights = None\n",
    "\n",
    "    def tokenize_sentences(self, sentences):\n",
    "        if self.dataset_name == 'SST1' or self.dataset_name == 'SST2':\n",
    "            return [self.clean_str_sst(sen).split(' ') for sen in sentences]\n",
    "        else:\n",
    "            return [self.clean_str(sen).split(' ') for sen in sentences]\n",
    "\n",
    "    def get_average_sentence_length(self, sentences):\n",
    "        sentences_tok = self.tokenize_sentences(sentences)\n",
    "        return np.mean([len(s) for s in sentences_tok])\n",
    "\n",
    "    def update_dict(self, sent):\n",
    "        for word in sent:\n",
    "            if not word.lower() in self.word2index:\n",
    "                self.wordCount += 1\n",
    "                self.word2index[word.lower()] = self.wordCount\n",
    "                self.index2word[self.wordCount] = word.lower()\n",
    "                self.wordcount[word.lower()] = 0\n",
    "            self.wordcount[word.lower()] += 1\n",
    "\n",
    "    def train_dictionary(self, sentences, use_pretrained_vector=False):\n",
    "        sentences_tok = self.tokenize_sentences(sentences)\n",
    "        for sent_tok in sentences_tok:\n",
    "            self.update_dict(sent_tok)\n",
    "\n",
    "        if use_pretrained_vector:\n",
    "            self.weights = np.zeros((self.wordCount + 1, self.word2vec.vector_size), np.float)\n",
    "            for i in self.index2word:\n",
    "                if self.index2word[i] in self.word2vec:\n",
    "                    self.weights[i, :] = self.word2vec[self.index2word[i]]\n",
    "                    self.wordCount_w2v += 1\n",
    "                elif self.wordcount[self.index2word[i]] >= 5:\n",
    "                    self.weights[i, :] = np.random.uniform(-0.25, 0.25, self.word2vec.vector_size)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            # Old method the weights were directly picked up from word2vec,\n",
    "            # weight values were not matching in th end thus skipped to self retaining weights\n",
    "            # self.word2index = {token: token_index for token_index, token in enumerate(self.word2vec.index2word)}\n",
    "            # self.index2word = {token_index: token for token_index, token in enumerate(self.word2vec.index2word)}\n",
    "            # self.update_wordCount_w2v()\n",
    "\n",
    "    def train_classes(self, classes):\n",
    "        for cl in np.unique(classes):\n",
    "            if cl not in self.class2index:\n",
    "                self.class2index[cl] = self.classCount\n",
    "                self.index2class[self.classCount] = cl\n",
    "                self.classCount += 1\n",
    "\n",
    "    def sent2Index(self, sentences):\n",
    "        sentIndexed = []\n",
    "        sentences_tok = self.tokenize_sentences(sentences)\n",
    "        for sent_tok in sentences_tok:\n",
    "            sentIndx = []\n",
    "            for w in sent_tok:\n",
    "                if w.lower() in self.word2index:\n",
    "                    sentIndx.append(self.word2index[w.lower()])\n",
    "                else:\n",
    "                    sentIndx.append(self.word2index['unk'])\n",
    "\n",
    "            if len(sentIndx) < self.max_sen_len:\n",
    "                sentIndx = sentIndx + ([self.word2index['unk']] * (self.max_sen_len - len(sentIndx)))\n",
    "            sentIndexed.append(sentIndx)\n",
    "\n",
    "            ## As per paper we initially used variable sentence length\n",
    "            ## adding 'unk' parameter only where it was necessary to achieve min filter\n",
    "            ## length. While similar accuracy is achieved by the method it lacks speed.\n",
    "            ## Missing out matrix multiplication as the modelling layer\n",
    "            # if len(sentIndx) < 5:\n",
    "            #     sentIndx = sentIndx + ([self.word2index['unk']] * (5 - len(sentIndx)))\n",
    "            # sentIndexed.append(torch.LongTensor(sentIndx).to(device))\n",
    "\n",
    "        return torch.LongTensor(sentIndexed).to(device)\n",
    "\n",
    "    def clean_str(self, sent):\n",
    "        sent = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sent)\n",
    "        sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n",
    "        sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n",
    "        sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n",
    "        sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n",
    "        sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n",
    "        sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n",
    "        sent = re.sub(r\",\", \" , \", sent)\n",
    "        sent = re.sub(r\"!\", \" ! \", sent)\n",
    "        sent = re.sub(r\"\\(\", \" ( \", sent)\n",
    "        sent = re.sub(r\"\\)\", \" ) \", sent)\n",
    "        sent = re.sub(r\"\\?\", \" ? \", sent)\n",
    "        sent = re.sub(r\"\\s{2,}\", \" \", sent)\n",
    "        return sent.strip().lower()\n",
    "\n",
    "    def clean_str_sst(self, sent):\n",
    "        sent = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sent)\n",
    "        sent = re.sub(r\"\\s{2,}\", \" \", sent)\n",
    "        return sent.strip().lower()\n",
    "\n",
    "    def class2Index(self, classList):\n",
    "        return torch.LongTensor([self.class2index[c] for c in classList]).to(device)\n",
    "\n",
    "    def proprocess_data(self, x, y):\n",
    "        x, y = shuffle(x, y, random_state=17)\n",
    "        x = self.sent2Index(x)\n",
    "        y = self.class2Index(y)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2vec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>No. of Classes</th>\n",
       "      <th>Average Length of Sentences</th>\n",
       "      <th>Max Length of Sentence</th>\n",
       "      <th>Dataset Size</th>\n",
       "      <th>Number of Words</th>\n",
       "      <th>Number of Words in Word2Vec</th>\n",
       "      <th>Test Data Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MR</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>56</td>\n",
       "      <td>10664</td>\n",
       "      <td>18779</td>\n",
       "      <td>16417</td>\n",
       "      <td>CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SST1</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>53</td>\n",
       "      <td>11286</td>\n",
       "      <td>17200</td>\n",
       "      <td>15748</td>\n",
       "      <td>2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SST2</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>53</td>\n",
       "      <td>9142</td>\n",
       "      <td>15603</td>\n",
       "      <td>14338</td>\n",
       "      <td>1749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subj</td>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>120</td>\n",
       "      <td>10000</td>\n",
       "      <td>21335</td>\n",
       "      <td>17897</td>\n",
       "      <td>CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TREC</td>\n",
       "      <td>6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>38</td>\n",
       "      <td>5952</td>\n",
       "      <td>8708</td>\n",
       "      <td>7475</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CR</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>105</td>\n",
       "      <td>4260</td>\n",
       "      <td>5226</td>\n",
       "      <td>4755</td>\n",
       "      <td>CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MPQA</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10606</td>\n",
       "      <td>6247</td>\n",
       "      <td>6084</td>\n",
       "      <td>CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset Name  No. of Classes  Average Length of Sentences  \\\n",
       "0           MR               2                         20.0   \n",
       "1         SST1               5                         18.0   \n",
       "2         SST2               2                         18.0   \n",
       "3         Subj               2                         23.0   \n",
       "4         TREC               6                         11.0   \n",
       "5           CR               2                         16.0   \n",
       "6         MPQA               2                          3.0   \n",
       "\n",
       "   Max Length of Sentence  Dataset Size  Number of Words  \\\n",
       "0                      56         10664            18779   \n",
       "1                      53         11286            17200   \n",
       "2                      53          9142            15603   \n",
       "3                     120         10000            21335   \n",
       "4                      38          5952             8708   \n",
       "5                     105          4260             5226   \n",
       "6                      36         10606             6247   \n",
       "\n",
       "   Number of Words in Word2Vec Test Data Size  \n",
       "0                        16417             CV  \n",
       "1                        15748           2125  \n",
       "2                        14338           1749  \n",
       "3                        17897             CV  \n",
       "4                         7475            500  \n",
       "5                         4755             CV  \n",
       "6                         6084             CV  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataview = {'Dataset Name': [],\n",
    "            'No. of Classes': [],\n",
    "            'Average Length of Sentences': [],\n",
    "            'Max Length of Sentence': [],\n",
    "            'Dataset Size': [],\n",
    "            'Number of Words': [],\n",
    "            'Number of Words in Word2Vec': [],\n",
    "            'Test Data Size': []}\n",
    "\n",
    "data_preprocessor = PreprocessData(use_pretrained_vector=True)\n",
    "for ds in ['MR', 'SST1', 'SST2', 'Subj', 'TREC', 'CR', 'MPQA']:\n",
    "    data_loader = LoadData(ds)\n",
    "    data_preprocessor.reset()\n",
    "    data_preprocessor.set_dataset_name(ds)\n",
    "    data_preprocessor.set_maximum_sentence_length(data_loader.data[0]['x_train'] + data_loader.data[0]['x_test'])\n",
    "    data_preprocessor.train_dictionary(data_loader.data[0]['x_train'] + data_loader.data[0]['x_test'], use_pretrained_vector=True)\n",
    "    data_preprocessor.train_classes(data_loader.data[0]['y_train'])\n",
    "    \n",
    "    dataview['Dataset Name'].append(ds)\n",
    "    dataview['No. of Classes'].append(data_preprocessor.classCount)\n",
    "    dataview['Average Length of Sentences'].append(np.round(data_preprocessor.get_average_sentence_length(data_loader.data[0]['x_train'] + \n",
    "                                                                                                 data_loader.data[0]['x_test']), 0))\n",
    "    dataview['Max Length of Sentence'].append(data_preprocessor.max_sen_len)\n",
    "    dataview['Dataset Size'].append(len(data_loader.data[0]['x_train'] + data_loader.data[0]['x_test']))\n",
    "    dataview['Number of Words'].append(data_preprocessor.wordCount)\n",
    "    dataview['Number of Words in Word2Vec'].append(data_preprocessor.wordCount_w2v)\n",
    "    dataview['Test Data Size'].append('CV' if len(data_loader.data) > 1 else len(data_loader.data[0]['x_test']))\n",
    "\n",
    "df = pd.DataFrame(dataview)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Data | c | l  |    N  |   V   |  Vpre | Test |\n",
    "|--------|---|----|-------|-------|-------|------|\n",
    "|   MR   | 2 | 20 | 10662 | 18765 | 16448 |  CV  |\n",
    "|  SST1  | 5 | 18 | 11855 | 17836 | 16262 | 2210 |\n",
    "|  SST2  | 2 | 19 | 9613  | 16185 | 14838 | 1821 |\n",
    "|  Subj  | 2 | 23 | 10000 | 21323 | 17913 |  CV  |\n",
    "|  TREC  | 6 | 10 | 5952  | 9592  | 9125  | 500  |\n",
    "|   CR   | 2 | 19 | 3775  | 5340  | 5046  |  CV  |\n",
    "|  MPQA  | 2 |  3 | 10606 | 6246  | 6083  |  CV  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CNNClassificationModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 use_pretrained_vector=False,\n",
    "                 word_count=300000,\n",
    "                 embedding_size=128,\n",
    "                 number_of_classes=2,\n",
    "                 batch_size=50,\n",
    "                 keep_embeddings_static=False,\n",
    "                 pretrained_vector_weight=None,\n",
    "                 use_multi_channel=False):\n",
    "        super(CNNClassificationModel, self).__init__()\n",
    "\n",
    "        self.keep_embeddings_static = keep_embeddings_static\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.input_channel = 1\n",
    "        self.use_multi_channel = use_multi_channel\n",
    "\n",
    "        # Setting up embeddings\n",
    "        if use_pretrained_vector:\n",
    "            self.embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_vector_weight),\n",
    "                                                                freeze=False)\n",
    "            self.embedding_size = pretrained_vector_weight.shape[1]\n",
    "        else:\n",
    "            self.embedding_layer = nn.Embedding(word_count, embedding_size)\n",
    "            self.embedding_size = embedding_size\n",
    "            nn.init.uniform_(self.embedding_layer.weight, -1.0, 1.0)\n",
    "\n",
    "        if use_multi_channel:\n",
    "            self.embedding_layer2 = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_vector_weight),\n",
    "                                                                 freeze=True)\n",
    "            self.input_channel = 2\n",
    "\n",
    "\n",
    "        self.convolution_layer_3dfilter = nn.Conv2d(self.input_channel, 100, (3, self.embedding_size))\n",
    "        nn.init.xavier_uniform_(self.convolution_layer_3dfilter.weight)\n",
    "\n",
    "        self.convolution_layer_4dfilter = nn.Conv1d(self.input_channel, 100, (4, self.embedding_size))\n",
    "        nn.init.xavier_uniform_(self.convolution_layer_4dfilter.weight)\n",
    "\n",
    "        self.convolution_layer_5dfilter = nn.Conv1d(self.input_channel, 100, (5, self.embedding_size))\n",
    "        nn.init.xavier_uniform_(self.convolution_layer_4dfilter.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear = nn.Linear(300, self.number_of_classes)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding_layer(input)\n",
    "        if self.keep_embeddings_static:\n",
    "            embedded = Variable(embedded)\n",
    "\n",
    "        if self.use_multi_channel:\n",
    "            embedded2 = self.embedding_layer2(input)\n",
    "            embedded = torch.stack([embedded, embedded2], dim=1)\n",
    "        else:\n",
    "            embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        conv_opt3 = self.convolution_layer_3dfilter(embedded)\n",
    "        conv_opt4 = self.convolution_layer_4dfilter(embedded)\n",
    "        conv_opt5 = self.convolution_layer_5dfilter(embedded)\n",
    "\n",
    "        conv_opt3 = nn.functional.relu(conv_opt3).squeeze(3)\n",
    "        conv_opt4 = nn.functional.relu(conv_opt4).squeeze(3)\n",
    "        conv_opt5 = nn.functional.relu(conv_opt5).squeeze(3)\n",
    "\n",
    "        conv_opt3 = nn.functional.max_pool1d(conv_opt3, conv_opt3.size(2)).squeeze(2)\n",
    "        conv_opt4 = nn.functional.max_pool1d(conv_opt4, conv_opt4.size(2)).squeeze(2)\n",
    "        conv_opt5 = nn.functional.max_pool1d(conv_opt5, conv_opt5.size(2)).squeeze(2)\n",
    "\n",
    "\n",
    "        conv_opt = torch.cat((conv_opt3, conv_opt4, conv_opt5), 1)\n",
    "        conv_opt = self.dropout(conv_opt)\n",
    "\n",
    "        linear_opt = self.linear(conv_opt)\n",
    "\n",
    "        return linear_opt\n",
    "\n",
    "    ## I used sentences with Variable  earlier as implemented in the paper but as the training was too slow\n",
    "    ## sentences were padded to maximum sentence length\n",
    "    # def forward(self, input):\n",
    "    #     conv_output = []\n",
    "    #     for inp in input:\n",
    "    #         embedded = self.embedding_layer(inp).view(1, self.embedding_size, -1)\n",
    "    #         if self.keep_embeddings_static:\n",
    "    #             embedded = Variable(embedded)\n",
    "    #         conv_opt3 = self.convolution_layer_3dfilter(embedded)\n",
    "    #         conv_opt4 = self.convolution_layer_4dfilter(embedded)\n",
    "    #         conv_opt5 = self.convolution_layer_5dfilter(embedded)\n",
    "    #         conv_opt3 = nn.functional.relu(conv_opt3)\n",
    "    #         conv_opt4 = nn.functional.relu(conv_opt4)\n",
    "    #         conv_opt5 = nn.functional.relu(conv_opt5)\n",
    "    #\n",
    "    #         # Maxpooling to take out the max from each one 100 fitera\n",
    "    #         conv_opt3 = nn.functional.max_pool1d(conv_opt3, conv_opt3.size(2))\n",
    "    #         conv_opt4 = nn.functional.max_pool1d(conv_opt4, conv_opt4.size(2))\n",
    "    #         conv_opt5 = nn.functional.max_pool1d(conv_opt5, conv_opt5.size(2))\n",
    "    #\n",
    "    #         conv_opt = torch.cat((conv_opt3, conv_opt4, conv_opt5), 2).view(1, -1)\n",
    "    #         conv_output.append(conv_opt)\n",
    "    #\n",
    "    #     conv_output = torch.cat(conv_output, 0)\n",
    "    #     conv_output = self.dropout(conv_output)\n",
    "    #\n",
    "    #     output = self.linear(conv_output)\n",
    "    #\n",
    "    #     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=50):\n",
    "    x, y = shuffle(x, y, random_state=13)\n",
    "    start_index, end_index = 0, 0\n",
    "    data_batches = []\n",
    "    while end_index < len(x):\n",
    "        end_index = (start_index + batch_size) if (start_index + batch_size) < len(x) else len(x)\n",
    "        data_batches.append((x[start_index:end_index], y[start_index:end_index]))\n",
    "        start_index = start_index + batch_size\n",
    "    return data_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, x_test, y_test,\n",
    "          model, optimizer, criterion,\n",
    "          model_name = 'model', model_store = False,\n",
    "          batch_size=50, epochs=100, log_iter=10):\n",
    "    \n",
    "    data_batches = get_batch(x_train, y_train, batch_size)\n",
    "    train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []\n",
    "    start_time = start_time = time.time()\n",
    "    for epoch in range(epochs):        \n",
    "        # Setting model intot training mode\n",
    "        model.train() # setting model in train mode\n",
    "        for batch_num, data in enumerate(data_batches):\n",
    "            x, y  = data[0], data[1]\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "#             This piece is to constrain the normalized weight ||w|| of \n",
    "#             the output linear layer be constrained at 3\n",
    "#             l2_weights = torch.norm(model.linear.weight).item()\n",
    "#             if l2_weights > 3:\n",
    "#                 model.linear.parameters = model.linear.weight/l2_weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            sys.stdout.write('\\rEpoch:{} | Batch:{} | Time Running: {}'.format(epoch, \n",
    "                                                                               batch_num, \n",
    "                                                                               datetime.timedelta(seconds=np.round(time.time() - start_time, 0))))\n",
    "\n",
    "            break\n",
    "            \n",
    "        trainloss, trainacc = evaluate(x_train, y_train, model, criterion)\n",
    "        testloss, testacc = evaluate(x_test, y_test, model, criterion)\n",
    "        \n",
    "        if epoch>log_iter and testacc > np.max(test_accuracy) and model_store == True:\n",
    "            torch.save(model, './models/' + model_name + '.torch')\n",
    "            \n",
    "        train_loss.append(trainloss)\n",
    "        train_accuracy.append(trainacc)\n",
    "        test_loss.append(testloss)\n",
    "        test_accuracy.append(testacc)\n",
    "        \n",
    "        if epoch%log_iter == 0:\n",
    "            print (' Train Acc {:.4f}, Train Loss {:.4f}, Test Acc {:.4f}, Test Loss {:.4f}'.format(trainacc, \n",
    "                                                                                                    trainloss, \n",
    "                                                                                                    testacc,\n",
    "                                                                                                    testloss))\n",
    "        \n",
    "                \n",
    "    print ('Accuracy Test {:.4f}'.format(np.max(test_accuracy)))\n",
    "    return train_loss, train_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x, y, model, criterion):\n",
    "    model.eval() # setting model to eval mode\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    accuracy = accuracy_score(y, y_pred.argmax(-1))\n",
    "    return loss.item(), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    model.eval() # setting model to eval mode\n",
    "    y_pred = model(x)    \n",
    "    return y_pred.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_results = {'MR': {}, 'SST1': {}, 'SST2': {}, 'Subj': {}, 'TREC': {}, 'CR': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Batch:0 | Time Running: 0:00:00 Train Acc 0.5060, Train Loss 0.7025, Test Acc 0.4995, Test Loss 0.7065\n",
      "Epoch:20 | Batch:0 | Time Running: 0:03:12 Train Acc 0.5452, Train Loss 0.7051, Test Acc 0.5380, Test Loss 0.7112\n",
      "Epoch:40 | Batch:0 | Time Running: 0:06:34 Train Acc 0.5480, Train Loss 0.7269, Test Acc 0.5211, Test Loss 0.7330\n",
      "Epoch:60 | Batch:0 | Time Running: 0:10:09 Train Acc 0.5482, Train Loss 0.7339, Test Acc 0.5342, Test Loss 0.7394\n",
      "Epoch:80 | Batch:0 | Time Running: 0:13:30 Train Acc 0.5483, Train Loss 0.7390, Test Acc 0.5380, Test Loss 0.7445\n",
      "Epoch:100 | Batch:0 | Time Running: 0:16:40 Train Acc 0.5484, Train Loss 0.7434, Test Acc 0.5323, Test Loss 0.7489\n",
      "Epoch:120 | Batch:0 | Time Running: 0:19:54 Train Acc 0.5482, Train Loss 0.7471, Test Acc 0.5333, Test Loss 0.7527\n",
      "Epoch:140 | Batch:0 | Time Running: 0:22:53 Train Acc 0.5497, Train Loss 0.7505, Test Acc 0.5398, Test Loss 0.7557\n",
      "Epoch:160 | Batch:0 | Time Running: 0:25:51 Train Acc 0.5508, Train Loss 0.7529, Test Acc 0.5351, Test Loss 0.7578\n",
      "Epoch:180 | Batch:0 | Time Running: 0:28:52 Train Acc 0.5501, Train Loss 0.7555, Test Acc 0.5389, Test Loss 0.7602\n",
      "Epoch:199 | Batch:0 | Time Running: 0:31:29Accuracy Test 0.5473\n",
      "CV Test Accuracy 0.5407685098406748\n"
     ]
    }
   ],
   "source": [
    "# data_preprocessor = PreprocessData()\n",
    "\n",
    "# MR\n",
    "data_loader = LoadData('MR')\n",
    "\n",
    "## CNN Rand\n",
    "accuracy = []\n",
    "for d in data_loader.data:\n",
    "    data_preprocessor.reset()\n",
    "    data_preprocessor.set_dataset_name(ds)\n",
    "    data_preprocessor.set_maximum_sentence_length(d['x_train'] + d['x_test'])\n",
    "    data_preprocessor.train_dictionary(d['x_train'] + d['x_test'])\n",
    "    data_preprocessor.train_classes(d['y_train'])\n",
    "    \n",
    "    x_train = data_preprocessor.sent2Index(d['x_train'])\n",
    "    y_train = data_preprocessor.class2Index(d['y_train'])\n",
    "    \n",
    "    x_test = data_preprocessor.sent2Index(d['x_test'])\n",
    "    y_test = data_preprocessor.class2Index(d['y_test'])\n",
    "    \n",
    "    model = CNNClassificationModel(use_pretrained_vector=False, \n",
    "                                   word_count=data_preprocessor.wordCount+1, \n",
    "                                   embedding_size=128,\n",
    "                                   number_of_classes=data_preprocessor.classCount,\n",
    "                                   batch_size=50, \n",
    "                                   keep_embeddings_static=False)\n",
    "    \n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_loss, train_accuracy, test_loss, test_accuracy = train(x_train, y_train, x_test, y_test, \n",
    "                                                                 model, optimizer, criterion, epochs=200, batch_size=50, \n",
    "                                                                 log_iter=20, model_name='MR', model_store=True)\n",
    "    \n",
    "    accuracy.append(test_accuracy[-1])\n",
    "    break\n",
    "\n",
    "print ('CV Test Accuracy {}'.format(np.mean(accuracy)))\n",
    "modelling_results['MR']['CNNRand'] = np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./results/MR.csv'):\n",
    "    df = pd.DataFrame({'Date': [], 'Model Type': [], 'Test Accuracy': [], 'Parameters': []})\n",
    "else:\n",
    "    df = pd.read_csv('./results/MR.csv')\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame({'Date': [datetime.datetime.now().strftime('%d %b %Y')],\n",
    "                                  'Model Type': ['CNN-Rand'],\n",
    "                                  'Test Accuracy': [np.mean(accuracy)],\n",
    "                                  'Parameters': []})])\n",
    "    \n",
    "df.to_csv('./results/MR.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
